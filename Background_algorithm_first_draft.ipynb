{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indian Buffet Process (IBP)\n",
    "Indian restaurants in London offer buffets with an apparently infinite number of dishes. Indian buffet process (IBP) is a distribution over infinite binary matrices, specifying how customers (objects) choose dishes (features).\n",
    "\n",
    "N customers enter a restaurant one after another. Each\n",
    "customer is exposed to a buffet consisting of infinitely many dishes arranged in a line. The first customer starts at the left of the buffet and takes a serving from each dish, stopping\n",
    "after a number of dishes which has a Poisson($\\alpha$) distribution. The ith customer moves along the buffet, sampling dishes in proportion to their popularity, taking dish k with probability $m_k$ , where $m_k$ is the number of previous customers who have sampled that dish. Having reached the end of all previous sampled dishes, the ith customer then tries a Poisson($\\frac{\\alpha}{i}$) number of new dishes. \n",
    "\n",
    "We can indicate which customers chose which dishes using a binary matrix Z with N rows and infinitely many columns, where $z_{ik}$ = 1 if the ith customer sampled the kth dish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linear-Gaussian binary latent feature model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our finite model, the $D$-dimensional vector of properties of an object i, $x_i$ is generated from a Gaussian distribution with mean $z_iA$ and covariance matrix $\\Sigma X = \\sigma^2_X $ **I**, where $z_i$ is a $K$-dimensional binary vector, and **A** is a $K$ $\\times$ $D$ matrix of weights. In matrix notation, E[**X**] = **ZA**. If **Z** is a feature matrix, this is a form of binary factor analysis. The distribution of **X** given **Z**, **A**, and $\\sigma$ **X** is matrix Gaussian with mean **ZA** and covariance matrix $\\sigma^2_X$ **I**, where I is the identity matrix. The prior on **A** is also matrix Gaussian, with mean 0 and covariance matrix $\\sigma^2_X$ **I**. Integrating out **A**, we have:\n",
    "\n",
    "\\begin{multline}\n",
    "P(X|Z,\\sigma_X, \\sigma_A) = \\frac{1}{(2 \\pi)^{ND/2} (\\sigma_X)^{(N-K)D}(\\sigma_A)^{KD}(|Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I|)^{D/2}}\\\\exp\\{-\\frac{1}{2\\sigma_X^2}tr(X^T(I-Z(Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I)^{-1}Z^T)X)\\}\n",
    "\\end{multline}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs sampling and Metropolis Hasting\n",
    "\n",
    "We will derive the posteriors analytically and update the following posteriors: feature binary matrix **Z**, number of new features $K_{new}$ and its parameter $\\alpha$, $\\sigma^2_X$ and $\\sigma^2_A$. And we will run Metropolis Algorithm on $\\sigma^2_X$, $\\sigma^2_A$, **Z**, and Gibbs sampling on $K_{new}$, $\\alpha$.\n",
    "\n",
    "##### Priors\n",
    "\n",
    "##### Gibbs samplers\n",
    "\n",
    "Full conditional posterior for Z is:\n",
    "\n",
    "\\begin{equation}\n",
    "P(z_{ik}|X,Z_{-(i,k),},\\sigma_X,\\sigma_A) \\propto  P(X|Z,\\sigma_X, \\sigma_A) * P(z_{ik}=1|\\textbf{z}_{-i,k}) \n",
    "\\end{equation}\n",
    "\n",
    "##### Metropolois Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
